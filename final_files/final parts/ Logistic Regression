---
title: "Logistic Regression"
author: "Roberto Diaz"
date: "15 de diciembre de 2017"
output: pdf_document
---

## 1.- LOADING DATASET
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(readr)
HR_comma_sep_v2 <- read_csv("~/Human_Resources_Analytics-Kaggle_DS/final_files/databases/HR_comma_sep_v2.csv")
```

To make a prediction, first, the database must be divided between a train and test databases.

## 2.- SPLIT TRAIN AND TEST DATABASE

# 2.1.- Spliting the dataset
```{r message=FALSE}
library(caret)
library(klaR)
```

Now we are going to define an 80%/20% train/test split of the dataset to have a part of the database that serves as control and another smaller part to test the model. 
```{r}
split<-0.80
trainIndex <- createDataPartition(HR_comma_sep_v2$left, p=split, list=FALSE)
train <- HR_comma_sep_v2[ trainIndex,]
test <- HR_comma_sep_v2[-trainIndex,]
```

# 3.- LOGISTIC REGRESSION

In statistics, logistic regression is a type of regression analysis used to predict the outcome of a categorical variable (a variable that can adopt a limited number of categories) based on independent or predictor variables.
First, we make the model, and see how it fit our data.

```{r}
logistic<-glm(left~., family=binomial(link='logit'), data=HR_comma_sep_v2)
summary(logistic)
```
We can see that the department variable is not significant (unless the department of RandD), but the rest of the variables are important.

```{r message=FALSE, warning=FALSE}
require(pscl)
pR2(logistic)
```
Here we can analyze the McFadden index, that is 0.40. Our model is very good.

## 3.1.- PREDICTION OF THE LOGISTIC REGRESSION
Now we are going to do the prediction of the test database.

```{r}
fitted.results <- predict(logistic, newdata=test, type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
```

And we will see the predictive accuracy of the model.
```{r}
misClasificError <- mean(fitted.results != test$left)
print(paste('Accuracy',1-misClasificError))
```
The predictive power is high, with an 87% rate of success.

```{r}
table(test$left, fitted.results, dnn = c('Actual Group','Predicted Group'))
```
## 3.2.- ROC CURVE

This graph is the representation of the ratio or ratio of true positives (VPR = True Positive Ratio) to the ratio or false positive rate (FPR = False Positive Ratio) also according to the discrimination threshold (value from of which we decided that a case is a positive).
We can use the ROC curve to analyze the predictive power.

```{r message=FALSE, warning=FALSE}
require(ROCR)
p <- predict(logistic, newdata=test, type="response")
pr <- prediction(p, test$left)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```

The next step is to analyze the area under the ROC curve, whose maximum is 1; therefore, when measuring the area under the curve, the closer it is to the greater the prediction the model will have.

```{r }
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

We can see here again that is a good predictive model cause the area under the Roc curve is closer to 1.

# 3.3.- PREDICTION WITH CROSS-VALIDATION

Now we are going to see the rate of success of the model with a Cross Validation, to have a more reliable result.

```{r}
library(caret)
HR_comma_sep_v2$left<-as.factor(HR_comma_sep_v2$left)
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(left~., data=HR_comma_sep_v2, trControl=train_control, method="glm")
print(model)
model$finalModel
```

Here we can see the coefficients of the different variables in the model, and the accuracy. It is on 87%, so it is a good model too. 

# 4.- TEST WITH THE SIGNIFICANT VARIABLES

Now we are going to remove the non significant variables to see if the model improves. We saw before that the department variable wasn't significant.

```{r warning=FALSE, message=FALSE}
require(dplyr)
baselog<-select(HR_comma_sep_v2, -department)
```


Now, we do exactly the same we did before with our new model.

```{r}
logistic2<-glm(left~., family=binomial(link='logit'), data=baselog)
summary(logistic2)
```

```{r}
pR2(logistic2)
```
But we can see that the McFadden index is lower, so it describes worse our data.

## 4.1.- PREDICTION OF THE LOGISTIC REGRESSION
```{r}
fitted.results2 <- predict(logistic2, newdata=test, type='response')
fitted.results2 <- ifelse(fitted.results > 0.5,1,0)
```

```{r}
misClasificError <- mean(fitted.results2 != test$left)
print(paste('Accuracy',1-misClasificError))
```
The accuracy this time is more or less the same.

```{r}
table(test$left, fitted.results2, dnn = c('Actual Group','Predicted Group'))
```
## 4.2.- ROC CURVE

We make again the ROC curve and the area under the curve. We can see that the accuracy of the model is similar to the model with all variables. 
```{r}
p <- predict(logistic2, newdata=test, type="response")
pr <- prediction(p, test$left)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```
```{r }
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

## 4.3.- PREDICTION WITH CROSS-VALIDATION

We make the cross-validation again: 
```{r}
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
model2 <- train(left~., data=baselog, trControl=train_control, method="glm")
print(model2)
model2$finalModel
```

And we can see that this model is sightly better than the former, so in this case, we could remove the "department" variable.
